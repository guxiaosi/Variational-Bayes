{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ab6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm4 gradient descent control variate, CODE Reference from VBayesLab\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import psi\n",
    "import math\n",
    "\n",
    "y = [11, 12, 8, 10, 9, 8, 9, 10, 13, 7] # data\n",
    "n = len(y)\n",
    "#===========================\n",
    "d = 4\n",
    "S = 300  #number of Monte Carlo samples\n",
    "beta1_adap_weight = 0.9 # adaptive learning weight\n",
    "beta2_adap_weight = 0.9 # adaptive learning weight\n",
    "eps0 = 0.1\n",
    "w_adadelta = 0.95 # adaptive learning weight\n",
    "eps_adadelta = 1e-7 # adaptive learning eps\n",
    "   \n",
    "max_iter = 2000\n",
    "patience_max = 20\n",
    "tau_threshold = max_iter/2\n",
    "t_w = 50\n",
    "#hyperparameter\n",
    "alpha_hp = 1\n",
    "beta_hp = 1\n",
    "mu_hp = 0\n",
    "sigma2_hp = 10\n",
    "\n",
    "lambdaq = np.array([np.mean(y),1.5,2,3]).reshape(1,4)# initial lambdaq\n",
    "lambda_best = lambdaq\n",
    "\n",
    "\n",
    "#function h_lambda_fun\n",
    "def h_lambda_fun(y,mu,sigma2,alpha_hp,beta_hp,mu_hp,sigma2_hp,mu_mu,sigma2_mu,alpha_sigma2,beta_sigma2):\n",
    "    log_p_mu = -1/2*np.log(2*np.pi)-1/2*np.log(sigma2_hp)-(mu-mu_hp)**2/2/sigma2_hp\n",
    "    log_p_sigma2 = alpha_hp*np.log(beta_hp)-np.log(math.gamma(alpha_hp))-(alpha_hp+1)*np.log(sigma2)-beta_hp/sigma2\n",
    "    log_q_mu = -1/2*np.log(2*np.pi)-1/2*np.log(sigma2_mu)-(mu-mu_mu)**2/2/sigma2_mu\n",
    "    log_q_sigma2 = alpha_sigma2*np.log(beta_sigma2)-np.log(math.gamma(alpha_sigma2))-(alpha_sigma2+1)*np.log(sigma2)-beta_sigma2/sigma2;\n",
    "    llh = -n/2*np.log(2*np.pi)-n/2*np.log(sigma2)-1/2/sigma2*sum((y-mu)**2)\n",
    "    return log_p_mu+log_p_sigma2+llh-log_q_mu-log_q_sigma2\n",
    "\n",
    "mu_mu = lambdaq[0,0]\n",
    "sigma2_mu = lambdaq[0,1]\n",
    "alpha_sigma2 = lambdaq[0,2]\n",
    "beta_sigma2 = lambdaq[0,3]\n",
    "h_lambda = np.zeros((S,1))#  function h_lambda\n",
    "grad_log_q_lambda = np.zeros((S,d))\n",
    "grad_log_q_times_h = np.zeros((S,d))\n",
    "for s in range(S):\n",
    "    mu = np.random.normal(mu_mu,math.sqrt(sigma2_mu),1)\n",
    "    sigma2 = 1./np.random.gamma(alpha_sigma2,1/beta_sigma2,1)\n",
    "\n",
    "    grad_log_q_lambda[s,:]=np.array([(mu-mu_mu)/sigma2_mu,\n",
    "                                     -1/2/sigma2_mu+(mu-mu_mu)**2/2/sigma2_mu**2,\n",
    "                                     np.log(beta_sigma2)-psi(alpha_sigma2)-np.log(sigma2),\n",
    "                                     alpha_sigma2/beta_sigma2-1/sigma2]).reshape(1,4)\n",
    "    h_lambda[s] = h_lambda_fun(y,mu,sigma2,alpha_hp,beta_hp,mu_hp,sigma2_hp,mu_mu,sigma2_mu,alpha_sigma2,beta_sigma2)    \n",
    "    grad_log_q_times_h[s,:] = grad_log_q_lambda[s,:]*h_lambda[s]\n",
    "\n",
    "cv = np.zeros((1,d))# control variate\n",
    "for i in range(d):\n",
    "    aa = np.cov(grad_log_q_times_h[:,i],grad_log_q_lambda[:,i])\n",
    "    cv[0,i] = aa[0,1]/aa[1,1]\n",
    "\n",
    "grad_LB1= np.mean(grad_log_q_times_h[:,0])\n",
    "grad_LB2= np.mean(grad_log_q_times_h[:,1])\n",
    "grad_LB3= np.mean(grad_log_q_times_h[:,2])\n",
    "grad_LB4= np.mean(grad_log_q_times_h[:,3])\n",
    "grad_LB =np.array([grad_LB1,grad_LB2,grad_LB3,grad_LB4]).reshape(1,4)\n",
    "\n",
    "#ADAM\n",
    "g_adaptive = grad_LB\n",
    "v_adaptive = g_adaptive**2\n",
    "g_bar_adaptive = g_adaptive\n",
    "v_bar_adaptive = v_adaptive\n",
    "\n",
    "iter_ = 1\n",
    "stop = False\n",
    "LB = np.array([-1e3 for i in range(2000)])\n",
    "LB_bar = np.array([-1e3 for i in range(2000)])\n",
    "patience = 0  \n",
    "\n",
    "while not stop:\n",
    "    mu_mu = lambdaq[0,0]\n",
    "    sigma2_mu = lambdaq[0,1]\n",
    "    alpha_sigma2 = lambdaq[0,2]\n",
    "    beta_sigma2 = lambdaq[0,3]\n",
    "    h_lambda = np.zeros((S,1))# function h_lambda\n",
    "    grad_log_q_lambda = np.zeros((S,d))\n",
    "    grad_log_q_times_h = np.zeros((S,d))\n",
    "    grad_log_q_times_h_cv = np.zeros((S,d))\n",
    "    for s in range(S):\n",
    "        mu = np.random.normal(mu_mu,math.sqrt(sigma2_mu),1)\n",
    "        sigma2 = 1./np.random.gamma(alpha_sigma2,1/beta_sigma2,1)\n",
    "\n",
    "        grad_log_q_lambda[s,:]=np.array([(mu-mu_mu)/sigma2_mu,\n",
    "                                     -1/2/sigma2_mu+(mu-mu_mu)**2/2/sigma2_mu**2,\n",
    "                                     np.log(beta_sigma2)-psi(alpha_sigma2)-np.log(sigma2),\n",
    "                                     alpha_sigma2/beta_sigma2-1/sigma2]).reshape(1,4)\n",
    "        h_lambda[s] = h_lambda_fun(y,mu,sigma2,alpha_hp,beta_hp,mu_hp,sigma2_hp,mu_mu,sigma2_mu,alpha_sigma2,beta_sigma2)    \n",
    "        grad_log_q_times_h[s,:] = grad_log_q_lambda[s,:]*h_lambda[s]\n",
    "        grad_log_q_times_h_cv[s,:] = grad_log_q_lambda[s,:]*(h_lambda[s]-cv)\n",
    "\n",
    "    cv = np.zeros((1,d))# control variate\n",
    "    for i in range(d):\n",
    "        aa = np.cov(grad_log_q_times_h[:,i],grad_log_q_lambda[:,i])\n",
    "        cv[0,i] = aa[0,1]/aa[1,1]\n",
    "    #print(\"CV-control variate: \" ,cv)\n",
    "    grad_LB1= np.mean(grad_log_q_times_h_cv[:,0])\n",
    "    grad_LB2= np.mean(grad_log_q_times_h_cv[:,1])\n",
    "    grad_LB3= np.mean(grad_log_q_times_h_cv[:,2])\n",
    "    grad_LB4= np.mean(grad_log_q_times_h_cv[:,3])\n",
    "    grad_LB =np.array([grad_LB1,grad_LB2,grad_LB3,grad_LB4]).reshape(1,4)\n",
    "\n",
    "    #case 'ADAM'\n",
    "    g_adaptive = grad_LB\n",
    "    v_adaptive = g_adaptive**2\n",
    "    g_bar_adaptive = beta1_adap_weight*g_bar_adaptive+(1-beta1_adap_weight)*g_adaptive\n",
    "    v_bar_adaptive = beta2_adap_weight*v_bar_adaptive+(1-beta2_adap_weight)*v_adaptive\n",
    "\n",
    "    if iter_>=tau_threshold:\n",
    "        stepsize = eps0*tau_threshold/iter_\n",
    "    else:\n",
    "        stepsize = eps0\n",
    "\n",
    "    lambdaq = lambdaq+stepsize*g_bar_adaptive/np.sqrt(v_bar_adaptive)\n",
    "\n",
    "    LB[iter_] = np.mean(h_lambda)\n",
    "\n",
    "    if iter_>=t_w:\n",
    "        LB_bar[iter_-t_w+1] = np.mean(LB[iter_-t_w+1:iter_])\n",
    "        LB_bar[iter_-t_w+1]\n",
    "\n",
    "\n",
    "    if iter_>t_w:\n",
    "        if LB_bar[iter_-t_w+1]>=max(LB_bar):\n",
    "            lambda_best = lambdaq\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience = patience+1\n",
    "\n",
    "    if patience>patience_max or iter_>max_iter:\n",
    "        stop = True\n",
    "    iter_ = iter_+1\n",
    "\n",
    "lambdaq = lambda_best\n",
    "mu_mu = lambdaq[0,0]\n",
    "sigma2_mu = lambdaq[0,1]\n",
    "alpha_sigma2 = lambdaq[0,2]\n",
    "beta_sigma2 = lambdaq[0,3]  \n",
    "print(\"lambdaq:\",lambdaq)\n",
    "print(\"Iterations: \",iter_)\n",
    "print(\"Evidence Lower Bound: \",LB[1:30])\n",
    "plt.plot(LB[1:150], color=\"b\",label=\"LB\")\n",
    "plt.plot(LB_bar[1:150], color=\"r\",label=\"LB_bar\")\n",
    "plt.title(\"ADAM LB_Samplers\") \n",
    "plt.xlabel(\"iterations\"); plt.ylabel('LB(Evidence Lower Bound)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299329f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
